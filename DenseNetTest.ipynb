{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vislab-001/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.transforms import transforms\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import ImageData\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"densenet\"\n",
    "model_image_size = 224\n",
    "# vit = models.vit_l_16(models.ViT_L_16_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(torch.nn.Module):\n",
    "    def __init__(self, numClasses: int, softmax:bool = True):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        self.densenetBase = torchvision.models.densenet121(weights='DEFAULT')\n",
    "        self.features = self.densenetBase.features\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.avgPool = torch.nn.AdaptiveMaxPool2d((1,1))\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 216),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(216, 4)\n",
    "        )\n",
    "        \n",
    "        for param in list(self.densenetBase.parameters())[:-1]:\n",
    "            param.requires_grad = True\n",
    "        # for param in self.inceptionBase.parameters():\n",
    "        #     print(param.requires_grad)\n",
    "\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = self.relu(features)\n",
    "        out = self.avgPool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet(4).to(device)\n",
    "# print(*list(model.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.RandomResizedCrop(size=(model_image_size, model_image_size), antialias=True), \n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainDataLoader, validDataLoader, testDataLoader, unseenDataLoader = ImageData.getImagesDataloaders(\"../ArtiFact/\", transforms = transform, batchSize=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = trainDataLoader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vislab-001/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/vislab-001/Documents/fmriMQP/test/SyntheticImageDetection/DenseNetTest.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vislab-001/Documents/fmriMQP/test/SyntheticImageDetection/DenseNetTest.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainIdxs, _ \u001b[39m=\u001b[39m train_test_split(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dataset\u001b[39m.\u001b[39mimagePaths)), train_size\u001b[39m=\u001b[39m\u001b[39m110\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "trainIdxs, _ = train_test_split(range(len(dataset.imagePaths)), train_size=110, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValIdxs = trainIdxs[100:]\n",
    "trainIdxs = trainIdxs[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainIdxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSub = Subset(dataset, trainIdxs)\n",
    "valSub = Subset(dataset, ValIdxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/vislab-001/Documents/fmriMQP/test/SyntheticImageDetection/DenseNetTest.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vislab-001/Documents/fmriMQP/test/SyntheticImageDetection/DenseNetTest.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainSubDataloader \u001b[39m=\u001b[39m DataLoader(trainSub, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vislab-001/Documents/fmriMQP/test/SyntheticImageDetection/DenseNetTest.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m valSubDataloader \u001b[39m=\u001b[39m DataLoader(valSub, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "trainSubDataloader = DataLoader(trainSub, batch_size=32, shuffle=True)\n",
    "valSubDataloader = DataLoader(valSub, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_data(model, dataloader, dirty: bool = False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        \n",
    "        num_correct = 0.0\n",
    "        num_correct_dirty = 0.0\n",
    "\n",
    "        num_samples = 0.0\n",
    "        for data in tqdm(dataloader, desc=\"Eval: \"):\n",
    "            image, label = data\n",
    "            label = label.to(device)\n",
    "            image = image.to(device)\n",
    "            outputs = model(image)\n",
    "            \n",
    "            dirtyLabel = torch.where(label > 1, torch.tensor(1, dtype = torch.int32).to(device), label)\n",
    "\n",
    "            \n",
    "            \n",
    "            loss = criterion(outputs, label)\n",
    "            total_loss += loss.item()\n",
    "            argMax = torch.argmax(outputs, 1)\n",
    "\n",
    "            # print(\"pred\")\n",
    "            # print(outputs)\n",
    "            # print(argMax)\n",
    "            # print(\"gt\")\n",
    "            # print(label)\n",
    "            for i in range(len(label)):\n",
    "                \n",
    "                if label[i] == argMax[i]:\n",
    "                    num_correct += 1\n",
    "\n",
    "                if (dirtyLabel[i] == argMax[i]) or (dirtyLabel[i] == 1 and argMax[i] > 0):\n",
    "                    num_correct_dirty += 1\n",
    "\n",
    "                num_samples += 1\n",
    "                    \n",
    "                \n",
    "                \n",
    "    return total_loss / len(dataloader), num_correct / num_samples, num_correct_dirty / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/117213 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 117213/117213 [6:27:04<00:00,  5.05it/s]  \n",
      "Eval: 100%|██████████| 6170/6170 [37:11<00:00,  2.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.3937, Valid Loss: 0.8057969989227901, Valid ACC: 0.6563598601894535, Dirty Valid ACC: 0.7070766425206423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 117213/117213 [8:06:26<00:00,  4.02it/s]  \n",
      "Eval: 100%|██████████| 6170/6170 [18:12<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Training Loss: 0.6292, Valid Loss: 0.6904794076297619, Valid ACC: 0.7244921736487513, Dirty Valid ACC: 0.7701838812623474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 117213/117213 [6:15:02<00:00,  5.21it/s]  \n",
      "Eval: 100%|██████████| 6170/6170 [18:07<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Training Loss: 0.6706, Valid Loss: 0.6375121697131393, Valid ACC: 0.7505394863482093, Dirty Valid ACC: 0.7885112203029229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 117213/117213 [6:13:53<00:00,  5.22it/s]  \n",
      "Eval: 100%|██████████| 6170/6170 [18:42<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Training Loss: 0.5124, Valid Loss: 0.5992763802264266, Valid ACC: 0.7653614305253027, Dirty Valid ACC: 0.7991084544855883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 117213/117213 [6:02:55<00:00,  5.38it/s]  \n",
      "Eval: 100%|██████████| 6170/6170 [13:41<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Training Loss: 0.9807, Valid Loss: 0.5748433977317192, Valid ACC: 0.7754217111595157, Dirty Valid ACC: 0.8047008763487159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 117213/117213 [6:03:18<00:00,  5.38it/s] \n",
      "Eval: 100%|██████████| 6170/6170 [13:47<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Training Loss: 0.2724, Valid Loss: 0.5412681179311326, Valid ACC: 0.7914087432247606, Dirty Valid ACC: 0.8262803302770882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 117213/117213 [6:02:51<00:00,  5.38it/s]  \n",
      "Eval: 100%|██████████| 6170/6170 [13:47<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Training Loss: 0.4825, Valid Loss: 0.5321472648319877, Valid ACC: 0.7957347652094625, Dirty Valid ACC: 0.8297452003444608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 2633/117213 [08:09<5:50:37,  5.45it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "count = 0\n",
    "valid_loss_array = np.zeros(num_epochs)\n",
    "valid_acc_array = np.zeros(num_epochs)\n",
    "valid_acc_dirty_array = np.zeros(num_epochs)\n",
    "\n",
    "train_loss_array = np.zeros(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "    batch_count = 0\n",
    "    for data in tqdm(trainDataLoader, desc=\"Training: \"):\n",
    "        \n",
    "        image, label = data\n",
    "        \n",
    "        label = label.to(device)\n",
    "        image = image.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "        # print(loss)\n",
    "            \n",
    "        \n",
    "    valid_loss, valid_acc, valid_acc_dirty = evaluate_on_data(model, validDataLoader)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}, Valid Loss: {valid_loss}, Valid ACC: {valid_acc}, Dirty Valid ACC: {valid_acc_dirty}')\n",
    "    valid_loss_array[epoch] = valid_loss\n",
    "    train_loss_array[epoch] = loss.item()\n",
    "    valid_acc_array[epoch] = valid_acc\n",
    "    valid_acc_dirty_array[epoch] = valid_acc_dirty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_acc_dirty = evaluate_on_data(model, testDataLoader)\n",
    "unseen_loss, unseen_acc, unseen_acc_dirty = evaluate_on_data(model, unseenDataLoader)\n",
    "\n",
    "testingData = {\n",
    "    \"test_loss\" : test_loss,\n",
    "    \"test_acc\" : test_acc,\n",
    "    \"test_acc_dirty\": test_acc_dirty,\n",
    "    \"unseen_loss\" : unseen_loss, \n",
    "    \"unseen_acc\" : unseen_acc, \n",
    "    \"unseen_acc_dirty\": unseen_acc_dirty\n",
    "}\n",
    "\n",
    "# Serializing json\n",
    "json_object = json.dumps(testingData, indent=4)\n",
    " \n",
    "# Writing to sample.json\n",
    "with open(f\"./results/{model_name}_testing.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/vislab-001/Documents/fmriMQP/test/SyntheticImageDetection/DenseNetTest.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vislab-001/Documents/fmriMQP/test/SyntheticImageDetection/DenseNetTest.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(model_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_valid_loss.npy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vislab-001/Documents/fmriMQP/test/SyntheticImageDetection/DenseNetTest.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     np\u001b[39m.\u001b[39msave(f, valid_loss_array)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vislab-001/Documents/fmriMQP/test/SyntheticImageDetection/DenseNetTest.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(model_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_valid_acc.npy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
     ]
    }
   ],
   "source": [
    "with open(f\"./results/{model_name}_valid_loss.npy\", 'wb') as f:\n",
    "    np.save(f, valid_loss_array)\n",
    "    \n",
    "with open(f\"./results/{model_name}_valid_acc.npy\", 'wb') as f:\n",
    "    np.save(f, valid_acc_array)\n",
    "\n",
    "with open(f\"./results/{model_name}_valid_dirty_acc.npy\", 'wb') as f:\n",
    "    np.save(f, valid_acc_dirty_array)\n",
    "    \n",
    "with open(f\"./results/{model_name}_train.npy\", 'wb') as f:\n",
    "    np.save(f, train_loss_array)\n",
    "\n",
    "torch.save(model.state_dict(), f\"./savedModels/{model_name}Params.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "8907f5995ab74a6cd5df9da2d2bcd12f57f5b23c9c38358337eeb837f01ad676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
